{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryaf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from utils import *\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ELECTRA model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [17:13<00:00, 426kB/s] \n",
      "c:\\Users\\aryaf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aryaf\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "# Load Electra Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\", num_labels=2)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electra.embeddings.word_embeddings.weight: False\n",
      "electra.embeddings.position_embeddings.weight: False\n",
      "electra.embeddings.token_type_embeddings.weight: False\n",
      "electra.embeddings.LayerNorm.weight: False\n",
      "electra.embeddings.LayerNorm.bias: False\n",
      "electra.encoder.layer.0.attention.self.query.weight: False\n",
      "electra.encoder.layer.0.attention.self.query.bias: False\n",
      "electra.encoder.layer.0.attention.self.key.weight: False\n",
      "electra.encoder.layer.0.attention.self.key.bias: False\n",
      "electra.encoder.layer.0.attention.self.value.weight: False\n",
      "electra.encoder.layer.0.attention.self.value.bias: False\n",
      "electra.encoder.layer.0.attention.output.dense.weight: False\n",
      "electra.encoder.layer.0.attention.output.dense.bias: False\n",
      "electra.encoder.layer.0.attention.output.LayerNorm.weight: False\n",
      "electra.encoder.layer.0.attention.output.LayerNorm.bias: False\n",
      "electra.encoder.layer.0.intermediate.dense.weight: False\n",
      "electra.encoder.layer.0.intermediate.dense.bias: False\n",
      "electra.encoder.layer.0.output.dense.weight: False\n",
      "electra.encoder.layer.0.output.dense.bias: False\n",
      "electra.encoder.layer.0.output.LayerNorm.weight: False\n",
      "electra.encoder.layer.0.output.LayerNorm.bias: False\n",
      "electra.encoder.layer.1.attention.self.query.weight: False\n",
      "electra.encoder.layer.1.attention.self.query.bias: False\n",
      "electra.encoder.layer.1.attention.self.key.weight: False\n",
      "electra.encoder.layer.1.attention.self.key.bias: False\n",
      "electra.encoder.layer.1.attention.self.value.weight: False\n",
      "electra.encoder.layer.1.attention.self.value.bias: False\n",
      "electra.encoder.layer.1.attention.output.dense.weight: False\n",
      "electra.encoder.layer.1.attention.output.dense.bias: False\n",
      "electra.encoder.layer.1.attention.output.LayerNorm.weight: False\n",
      "electra.encoder.layer.1.attention.output.LayerNorm.bias: False\n",
      "electra.encoder.layer.1.intermediate.dense.weight: False\n",
      "electra.encoder.layer.1.intermediate.dense.bias: False\n",
      "electra.encoder.layer.1.output.dense.weight: False\n",
      "electra.encoder.layer.1.output.dense.bias: False\n",
      "electra.encoder.layer.1.output.LayerNorm.weight: False\n",
      "electra.encoder.layer.1.output.LayerNorm.bias: False\n",
      "electra.encoder.layer.2.attention.self.query.weight: False\n",
      "electra.encoder.layer.2.attention.self.query.bias: False\n",
      "electra.encoder.layer.2.attention.self.key.weight: False\n",
      "electra.encoder.layer.2.attention.self.key.bias: False\n",
      "electra.encoder.layer.2.attention.self.value.weight: False\n",
      "electra.encoder.layer.2.attention.self.value.bias: False\n",
      "electra.encoder.layer.2.attention.output.dense.weight: False\n",
      "electra.encoder.layer.2.attention.output.dense.bias: False\n",
      "electra.encoder.layer.2.attention.output.LayerNorm.weight: False\n",
      "electra.encoder.layer.2.attention.output.LayerNorm.bias: False\n",
      "electra.encoder.layer.2.intermediate.dense.weight: False\n",
      "electra.encoder.layer.2.intermediate.dense.bias: False\n",
      "electra.encoder.layer.2.output.dense.weight: False\n",
      "electra.encoder.layer.2.output.dense.bias: False\n",
      "electra.encoder.layer.2.output.LayerNorm.weight: False\n",
      "electra.encoder.layer.2.output.LayerNorm.bias: False\n",
      "electra.encoder.layer.3.attention.self.query.weight: True\n",
      "electra.encoder.layer.3.attention.self.query.bias: True\n",
      "electra.encoder.layer.3.attention.self.key.weight: True\n",
      "electra.encoder.layer.3.attention.self.key.bias: True\n",
      "electra.encoder.layer.3.attention.self.value.weight: True\n",
      "electra.encoder.layer.3.attention.self.value.bias: True\n",
      "electra.encoder.layer.3.attention.output.dense.weight: True\n",
      "electra.encoder.layer.3.attention.output.dense.bias: True\n",
      "electra.encoder.layer.3.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.3.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.3.intermediate.dense.weight: True\n",
      "electra.encoder.layer.3.intermediate.dense.bias: True\n",
      "electra.encoder.layer.3.output.dense.weight: True\n",
      "electra.encoder.layer.3.output.dense.bias: True\n",
      "electra.encoder.layer.3.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.3.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.4.attention.self.query.weight: True\n",
      "electra.encoder.layer.4.attention.self.query.bias: True\n",
      "electra.encoder.layer.4.attention.self.key.weight: True\n",
      "electra.encoder.layer.4.attention.self.key.bias: True\n",
      "electra.encoder.layer.4.attention.self.value.weight: True\n",
      "electra.encoder.layer.4.attention.self.value.bias: True\n",
      "electra.encoder.layer.4.attention.output.dense.weight: True\n",
      "electra.encoder.layer.4.attention.output.dense.bias: True\n",
      "electra.encoder.layer.4.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.4.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.4.intermediate.dense.weight: True\n",
      "electra.encoder.layer.4.intermediate.dense.bias: True\n",
      "electra.encoder.layer.4.output.dense.weight: True\n",
      "electra.encoder.layer.4.output.dense.bias: True\n",
      "electra.encoder.layer.4.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.4.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.5.attention.self.query.weight: True\n",
      "electra.encoder.layer.5.attention.self.query.bias: True\n",
      "electra.encoder.layer.5.attention.self.key.weight: True\n",
      "electra.encoder.layer.5.attention.self.key.bias: True\n",
      "electra.encoder.layer.5.attention.self.value.weight: True\n",
      "electra.encoder.layer.5.attention.self.value.bias: True\n",
      "electra.encoder.layer.5.attention.output.dense.weight: True\n",
      "electra.encoder.layer.5.attention.output.dense.bias: True\n",
      "electra.encoder.layer.5.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.5.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.5.intermediate.dense.weight: True\n",
      "electra.encoder.layer.5.intermediate.dense.bias: True\n",
      "electra.encoder.layer.5.output.dense.weight: True\n",
      "electra.encoder.layer.5.output.dense.bias: True\n",
      "electra.encoder.layer.5.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.5.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.6.attention.self.query.weight: True\n",
      "electra.encoder.layer.6.attention.self.query.bias: True\n",
      "electra.encoder.layer.6.attention.self.key.weight: True\n",
      "electra.encoder.layer.6.attention.self.key.bias: True\n",
      "electra.encoder.layer.6.attention.self.value.weight: True\n",
      "electra.encoder.layer.6.attention.self.value.bias: True\n",
      "electra.encoder.layer.6.attention.output.dense.weight: True\n",
      "electra.encoder.layer.6.attention.output.dense.bias: True\n",
      "electra.encoder.layer.6.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.6.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.6.intermediate.dense.weight: True\n",
      "electra.encoder.layer.6.intermediate.dense.bias: True\n",
      "electra.encoder.layer.6.output.dense.weight: True\n",
      "electra.encoder.layer.6.output.dense.bias: True\n",
      "electra.encoder.layer.6.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.6.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.7.attention.self.query.weight: True\n",
      "electra.encoder.layer.7.attention.self.query.bias: True\n",
      "electra.encoder.layer.7.attention.self.key.weight: True\n",
      "electra.encoder.layer.7.attention.self.key.bias: True\n",
      "electra.encoder.layer.7.attention.self.value.weight: True\n",
      "electra.encoder.layer.7.attention.self.value.bias: True\n",
      "electra.encoder.layer.7.attention.output.dense.weight: True\n",
      "electra.encoder.layer.7.attention.output.dense.bias: True\n",
      "electra.encoder.layer.7.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.7.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.7.intermediate.dense.weight: True\n",
      "electra.encoder.layer.7.intermediate.dense.bias: True\n",
      "electra.encoder.layer.7.output.dense.weight: True\n",
      "electra.encoder.layer.7.output.dense.bias: True\n",
      "electra.encoder.layer.7.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.7.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.8.attention.self.query.weight: True\n",
      "electra.encoder.layer.8.attention.self.query.bias: True\n",
      "electra.encoder.layer.8.attention.self.key.weight: True\n",
      "electra.encoder.layer.8.attention.self.key.bias: True\n",
      "electra.encoder.layer.8.attention.self.value.weight: True\n",
      "electra.encoder.layer.8.attention.self.value.bias: True\n",
      "electra.encoder.layer.8.attention.output.dense.weight: True\n",
      "electra.encoder.layer.8.attention.output.dense.bias: True\n",
      "electra.encoder.layer.8.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.8.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.8.intermediate.dense.weight: True\n",
      "electra.encoder.layer.8.intermediate.dense.bias: True\n",
      "electra.encoder.layer.8.output.dense.weight: True\n",
      "electra.encoder.layer.8.output.dense.bias: True\n",
      "electra.encoder.layer.8.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.8.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.9.attention.self.query.weight: True\n",
      "electra.encoder.layer.9.attention.self.query.bias: True\n",
      "electra.encoder.layer.9.attention.self.key.weight: True\n",
      "electra.encoder.layer.9.attention.self.key.bias: True\n",
      "electra.encoder.layer.9.attention.self.value.weight: True\n",
      "electra.encoder.layer.9.attention.self.value.bias: True\n",
      "electra.encoder.layer.9.attention.output.dense.weight: True\n",
      "electra.encoder.layer.9.attention.output.dense.bias: True\n",
      "electra.encoder.layer.9.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.9.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.9.intermediate.dense.weight: True\n",
      "electra.encoder.layer.9.intermediate.dense.bias: True\n",
      "electra.encoder.layer.9.output.dense.weight: True\n",
      "electra.encoder.layer.9.output.dense.bias: True\n",
      "electra.encoder.layer.9.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.9.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.10.attention.self.query.weight: True\n",
      "electra.encoder.layer.10.attention.self.query.bias: True\n",
      "electra.encoder.layer.10.attention.self.key.weight: True\n",
      "electra.encoder.layer.10.attention.self.key.bias: True\n",
      "electra.encoder.layer.10.attention.self.value.weight: True\n",
      "electra.encoder.layer.10.attention.self.value.bias: True\n",
      "electra.encoder.layer.10.attention.output.dense.weight: True\n",
      "electra.encoder.layer.10.attention.output.dense.bias: True\n",
      "electra.encoder.layer.10.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.10.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.10.intermediate.dense.weight: True\n",
      "electra.encoder.layer.10.intermediate.dense.bias: True\n",
      "electra.encoder.layer.10.output.dense.weight: True\n",
      "electra.encoder.layer.10.output.dense.bias: True\n",
      "electra.encoder.layer.10.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.10.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.11.attention.self.query.weight: True\n",
      "electra.encoder.layer.11.attention.self.query.bias: True\n",
      "electra.encoder.layer.11.attention.self.key.weight: True\n",
      "electra.encoder.layer.11.attention.self.key.bias: True\n",
      "electra.encoder.layer.11.attention.self.value.weight: True\n",
      "electra.encoder.layer.11.attention.self.value.bias: True\n",
      "electra.encoder.layer.11.attention.output.dense.weight: True\n",
      "electra.encoder.layer.11.attention.output.dense.bias: True\n",
      "electra.encoder.layer.11.attention.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.11.attention.output.LayerNorm.bias: True\n",
      "electra.encoder.layer.11.intermediate.dense.weight: True\n",
      "electra.encoder.layer.11.intermediate.dense.bias: True\n",
      "electra.encoder.layer.11.output.dense.weight: True\n",
      "electra.encoder.layer.11.output.dense.bias: True\n",
      "electra.encoder.layer.11.output.LayerNorm.weight: True\n",
      "electra.encoder.layer.11.output.LayerNorm.bias: True\n",
      "classifier.dense.weight: True\n",
      "classifier.dense.bias: True\n",
      "classifier.out_proj.weight: True\n",
      "classifier.out_proj.bias: True\n"
     ]
    }
   ],
   "source": [
    "# Freeze All layer except last layer and last 2 block\n",
    "unfreeze_last_layer_last_9block(model)\n",
    "\n",
    "print_layer(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset SST-2 English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_data = pd.read_csv(\"SST-2_datasets/train.tsv\", delimiter='\\t', names=['labels','sentence'])\n",
    "val_data = pd.read_csv(\"SST-2_datasets/dev.tsv\", delimiter='\\t', names=['labels','sentence'])\n",
    "test_data = pd.read_csv(\"SST-2_datasets/test.tsv\", delimiter='\\t', names=['labels','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_data = encoded_data(tokenizer, train_data)\n",
    "val_encoded_data = encoded_data(tokenizer, val_data)\n",
    "test_encoded_data = encoded_data(tokenizer, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_encoded_data, train_data, device)\n",
    "val_dataset = create_dataset(val_encoded_data, val_data, device)\n",
    "test_dataset = create_dataset(test_encoded_data, test_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and learning rate scheduler\n",
    "num_epochs = 5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=6.68561343998775e-5, eps=1e-8)\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------\n",
      "Epoch 1 / 5\n",
      "Training loss:  0.044461920857429504\n",
      "Validation loss:  0.2019630494926657\n",
      "Validation accuracy:  0.926605504587156\n",
      "\n",
      "--------------------------------------------\n",
      "Epoch 2 / 5\n",
      "Training loss:  0.007444287650287151\n",
      "Validation loss:  0.1705740226233112\n",
      "Validation accuracy:  0.9403669724770642\n",
      "\n",
      "--------------------------------------------\n",
      "Epoch 3 / 5\n",
      "Training loss:  0.002492310479283333\n",
      "Validation loss:  0.18429338436440698\n",
      "Validation accuracy:  0.9541284403669725\n",
      "\n",
      "--------------------------------------------\n",
      "Epoch 4 / 5\n",
      "Training loss:  0.0007966127595864236\n",
      "Validation loss:  0.21045066969860013\n",
      "Validation accuracy:  0.9529816513761468\n",
      "\n",
      "--------------------------------------------\n",
      "Epoch 5 / 5\n",
      "Training loss:  0.0006084076012484729\n",
      "Validation loss:  0.21496401263824996\n",
      "Validation accuracy:  0.9495412844036697\n",
      "Time 1007.3376495838165\n"
     ]
    }
   ],
   "source": [
    "# Training Electra model\n",
    "train_loss = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "start = time.time()\n",
    "trainer(train_loss, val_losses, val_accuracies, num_epochs, train_loader, val_dataset, val_loader, model, optimizer, scheduler, device)\n",
    "print(\"Time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'electra_models/transformerELECTRA-11.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reload model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/electra-base-discriminator\", num_labels=2)\n",
    "model.load_state_dict(torch.load('electra_models/transformerELECTRA-11.pt', map_location=device))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.22678174592836417\n",
      "Test accuracy: 94.73%\n",
      "Time 25.322784185409546\n"
     ]
    }
   ],
   "source": [
    "testing(model, test_loader, test_data, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
